# @package _global_
module:
  _target_: emg2qwerty.lightning.TransformerCTCModule
  in_features: 528  # freq * channels = (n_fft // 2 + 1) * 16
  d_model: 256  # Small model dimension
  nhead: 4  # Number of attention heads
  num_layers: 3  # Number of transformer layers
  dim_feedforward: 512  # Dimension of feedforward network
  dropout: 0.1
  activation: "relu"
  positional_encoding: true
  max_seq_length: 150000  # Increased maximum sequence length for positional encoding

# Define optimizer explicitly for this model
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
  weight_decay: 0.0001

# Define a simpler learning rate scheduler that works with direct optimizer passing
# Use _target_global_ to completely override the base configuration
lr_scheduler:
  _target_global_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: min
  patience: 2
  factor: 0.5
  # Add required parameters for PyTorch Lightning
  interval: epoch
  monitor: val/loss

# Override trainer configuration to set max_epochs to 70
trainer:
  max_epochs: 70

datamodule:
  _target_: emg2qwerty.lightning.WindowedEMGDataModule
  window_length: 8000  # 4 sec windows for 2kHz EMG
  padding: [1800, 200]  # 900ms past context, 100ms future context 